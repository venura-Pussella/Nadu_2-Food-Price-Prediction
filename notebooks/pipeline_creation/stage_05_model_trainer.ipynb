{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\VenuraP\\\\Desktop\\\\Browns Data Projects\\\\ML Projects\\\\POC\\\\Harti-Food-Price-Prediction'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Class for Configuration Entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    local_data_path: Path\n",
    "    train_y_data_file: Path\n",
    "    train_x_data_file: Path\n",
    "    test_y_data_file: Path\n",
    "    test_x_data_file: Path\n",
    "    model_checkpoint_path: Path\n",
    "    model_name: str\n",
    "    n_units_layer1: int\n",
    "    n_units_layer2: int\n",
    "    n_units_layer3: int\n",
    "    dropout_rate: float\n",
    "    sequence_length: int\n",
    "    optimizer: str\n",
    "    loss_function: str\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    validation_split: float\n",
    "    target_column: str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to initialize configuration by reading YAML and creating directories\n",
    "def load_configuration(config_filepath: Path = CONFIG_FILE_PATH, schema_filepath: Path = SCHEMA_FILE_PATH):\n",
    "\n",
    "    config = read_yaml(config_filepath)\n",
    "    schema = read_yaml(schema_filepath)\n",
    "\n",
    "    return config, schema\n",
    "\n",
    "# Function to get data ingestion configuration from the loaded config\n",
    "def get_model_trainer_config(config,schema) -> ModelTrainerConfig:\n",
    "\n",
    "    # Extract data ingestion settings from the config\n",
    "    model_trainer = config.model_trainer\n",
    "\n",
    "    # Create and return a DataIngestionConfig instance\n",
    "    return ModelTrainerConfig(\n",
    "        root_dir=model_trainer.root_dir,\n",
    "        local_data_path=model_trainer.local_data_path,\n",
    "        train_x_data_file=model_trainer.train_x_data_file,\n",
    "        train_y_data_file=model_trainer.train_y_data_file,\n",
    "        test_x_data_file=model_trainer.test_x_data_file,\n",
    "        test_y_data_file=model_trainer.test_y_data_file,\n",
    "        model_checkpoint_path=model_trainer.model_checkpoint_path,\n",
    "        model_name=model_trainer.model_name,\n",
    "        n_units_layer1=model_trainer.n_units_layer1,\n",
    "        n_units_layer2=model_trainer.n_units_layer2,\n",
    "        n_units_layer3=model_trainer.n_units_layer3,\n",
    "        dropout_rate=model_trainer.dropout_rate,\n",
    "        sequence_length=model_trainer.sequence_length,\n",
    "        optimizer=model_trainer.optimizer,\n",
    "        loss_function=model_trainer.loss_function,\n",
    "        epochs=model_trainer.epochs,\n",
    "        batch_size=model_trainer.batch_size,\n",
    "        validation_split=model_trainer.validation_split,\n",
    "        target_column = schema.TARGET_COLUMN\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Trainer Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def sequence_creation_train_test_split(config):\n",
    "    # Read the Excel file into a DataFrame\n",
    "    df = pd.read_excel(config.local_data_path)\n",
    "    \n",
    "    # Define sequence length and number of features\n",
    "    sequence_length = config.sequence_length \n",
    "    num_features = len(df.columns)\n",
    "\n",
    "    # Create sequences and corresponding labels\n",
    "    sequences = []\n",
    "    labels = []\n",
    "\n",
    "    # Dynamically set target column based on config.target_column\n",
    "    target_col_idx = df.columns.get_loc(config.target_column)  # Get index of target column\n",
    "\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df.iloc[i:i+sequence_length, :]\n",
    "        label = df.iloc[i+sequence_length, target_col_idx]  # since target is the first column\n",
    "        sequences.append(seq)\n",
    "        labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    sequences = np.array(sequences)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Split into train and test sets\n",
    "    train_size = int(0.8 * len(sequences)) \n",
    "    train_x, test_x = sequences[:train_size], sequences[train_size:]\n",
    "    train_y, test_y = labels[:train_size], labels[train_size:]\n",
    "\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "\n",
    "def save_train_test_data_to_excel(train_x, test_x, train_y, test_y, config):\n",
    "\n",
    "    # Convert train_x, train_y, test_x, test_y to DataFrames\n",
    "    train_x_df = pd.DataFrame(train_x.reshape(train_x.shape[0], -1))  # Flatten 3D array to 2D\n",
    "    train_y_df = pd.DataFrame(train_y, columns=[config.target_column])\n",
    "\n",
    "    test_x_df = pd.DataFrame(test_x.reshape(test_x.shape[0], -1))  # Flatten 3D array to 2D\n",
    "    test_y_df = pd.DataFrame(test_y, columns=[config.target_column])\n",
    "\n",
    "    # Save DataFrames to Excel files defined in config\n",
    "    train_x_path = config.train_x_data_file\n",
    "    train_y_path = config.train_y_data_file\n",
    "    test_x_path = config.test_x_data_file\n",
    "    test_y_path = config.test_y_data_file\n",
    "\n",
    "    train_x_df.to_excel(train_x_path, index=False)\n",
    "    train_y_df.to_excel(train_y_path, index=False)\n",
    "\n",
    "    test_x_df.to_excel(test_x_path, index=False)\n",
    "    test_y_df.to_excel(test_y_path, index=False)\n",
    "\n",
    "    print(f\"Train and test data saved to Excel files:\\n\"\n",
    "          f\"Train X: {train_x_path}\\nTrain Y: {train_y_path}\\n\"\n",
    "          f\"Test X: {test_x_path}\\nTest Y: {test_y_path}\")\n",
    "\n",
    "\n",
    "def lstm_model_trainer(train_x, train_y, config):\n",
    "\n",
    "    # Create the LSTM model using the provided config\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add Bidirectional LSTM layers with dropout\n",
    "    model.add(Bidirectional(LSTM(units=config.n_units_layer1, return_sequences=True), \n",
    "                            input_shape=(train_x.shape[1], train_x.shape[2])))\n",
    "    model.add(Dropout(config.dropout_rate))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=config.n_units_layer2, return_sequences=True)))\n",
    "    model.add(Dropout(config.dropout_rate))\n",
    "\n",
    "    model.add(Bidirectional(LSTM(units=config.n_units_layer3, return_sequences=False)))\n",
    "    model.add(Dropout(config.dropout_rate))\n",
    "\n",
    "    # Add a dense output layer\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    # Compile the model using the config\n",
    "    model.compile(optimizer=config.optimizer, loss=config.loss_function)\n",
    "\n",
    "    # Create a checkpoint callback for saving the best model\n",
    "    model_checkpoint = ModelCheckpoint(filepath=str(config.model_checkpoint_path), \n",
    "                                       monitor='val_loss', \n",
    "                                       save_best_only=True)\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_x, train_y,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        validation_split=config.validation_split,\n",
    "        callbacks=[model_checkpoint]\n",
    "    )\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Trainer Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-10-03 17:35:13,636: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-10-03 17:35:13,637: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2024-10-03 17:35:13,637: INFO: common: created directory at: artifacts/model_trainer]\n",
      "[2024-10-03 17:35:14,051: ERROR: 3964945739: An error occurred in model trainer : {'name': 'pettah_average'}]\n"
     ]
    }
   ],
   "source": [
    "from src import logger\n",
    "\n",
    "def model_training_pipeline():\n",
    "    try:\n",
    "        # Load config and schema\n",
    "        config,schema = load_configuration()\n",
    "\n",
    "        # Retrieve the data ingestion configuration from the loaded config\n",
    "        model_trainer_config = get_model_trainer_config(config,schema)\n",
    "\n",
    "        # Create directories related to data ingestion (root directory)\n",
    "        create_directories([model_trainer_config.root_dir])\n",
    "\n",
    "        # Sequence creation\n",
    "        train_x, test_x, train_y, test_y = sequence_creation_train_test_split(model_trainer_config)\n",
    "        \n",
    "        # Saving data to an excel from numpy array (3D to 2D)\n",
    "        save_train_test_data_to_excel(train_x, test_x, train_y, test_y, config)\n",
    "\n",
    "        # Train the model\n",
    "        lstm_model_trainer(train_x, train_y, config)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred in model trainer : {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    " model_training_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hartipredict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
